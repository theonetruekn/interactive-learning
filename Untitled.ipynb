{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2373888b-25a8-43fd-a0bd-4e47bab1e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_string(json_string):\n",
    "    try:\n",
    "        # Remove the '--- END OF LIST ---' if it's present\n",
    "        if '--- END OF LIST ---' in json_string:\n",
    "            json_string = json_string.replace('--- END OF LIST ---', '').strip()\n",
    "        \n",
    "        # Attempt to parse the JSON string into a Python object\n",
    "        data = json.loads(json_string)\n",
    "        \n",
    "        # Check if the top-level structure is a list\n",
    "        if not isinstance(data, list):\n",
    "            return (False, \"Error: JSON should start with a list of dictionaries.\")\n",
    "        \n",
    "        for i, item in enumerate(data):\n",
    "            if not isinstance(item, dict):\n",
    "                return (False, f\"Error: Element at index {i} should be a dictionary.\")\n",
    "            \n",
    "            # Check for the 'file_path' key and its type\n",
    "            if 'file_path' not in item:\n",
    "                return (False, f\"Error: Missing 'file_path' key in element at index {i}.\")\n",
    "            if not isinstance(item['file_path'], str):\n",
    "                return (False, f\"Error: 'file_path' at index {i} should be a string.\")\n",
    "            \n",
    "            # Check for 'selected_functions' and 'selected_classes' keys and their types\n",
    "            if 'selected_functions' not in item or not isinstance(item['selected_functions'], list):\n",
    "                return (False, f\"Error: 'selected_functions' at index {i} should be a list.\")\n",
    "            if 'selected_classes' not in item or not isinstance(item['selected_classes'], list):\n",
    "                return (False, f\"Error: 'selected_classes' at index {i} should be a list.\")\n",
    "            \n",
    "            # Convert all elements in 'selected_functions' and 'selected_classes' to strings\n",
    "            item['selected_functions'] = [str(func) for func in item['selected_functions']]\n",
    "            item['selected_classes'] = [str(cls) for cls in item['selected_classes']]\n",
    "        \n",
    "        return (True, data)\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        return (False, f\"Error: Failed to parse JSON. {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a72be125-b348-407e-84e4-d327e8c2ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b296513-4219-4e33-9032-6eb7cc58253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test =\"\"\"\n",
    "[\n",
    "    {\n",
    "        \"file_path\": \"./repos/sqlfluff/src/sqlfluff/core/config.py\",\n",
    "        \"selected_functions\": [],\n",
    "        \"selected_classes\": [\n",
    "            \"ConfigLoader\",\n",
    "            \"FluffConfig\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file_path\": \"./repos/sqlfluff/src/sqlfluff/core/linter/linted_file.py\",\n",
    "        \"selected_functions\": [],\n",
    "        \"selected_classes\": [\n",
    "            \"LintedFile\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file_path\": \"./repos/sqlfluff/src/sqlfluff/core/parser/lexer.py\",\n",
    "        \"selected_functions\": [],\n",
    "        \"selected_classes\": [\n",
    "            \"TemplateElement\",\n",
    "            \"LexMatch\",\n",
    "            \"StringLexer\",\n",
    "            \"RegexLexer\",\n",
    "            \"Lexer\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file_path\": \"./repos/sqlfluff/src/sqlfluff/core/parser/matchable.py\",\n",
    "        \"selected_functions\": [\"match\"],\n",
    "        \"selected_classes\": [\n",
    "            \"Matchable\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "--- END OF LIST ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbdabed8-a86f-4323-a5fe-4f983effda84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 225 entries, 0 to 224\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   repo                      225 non-null    object             \n",
      " 1   instance_id               225 non-null    object             \n",
      " 2   base_commit               225 non-null    object             \n",
      " 3   patch                     225 non-null    object             \n",
      " 4   test_patch                225 non-null    object             \n",
      " 5   problem_statement         225 non-null    object             \n",
      " 6   hints_text                225 non-null    object             \n",
      " 7   created_at                225 non-null    datetime64[ns, UTC]\n",
      " 8   version                   225 non-null    float64            \n",
      " 9   FAIL_TO_PASS              225 non-null    object             \n",
      " 10  PASS_TO_PASS              225 non-null    object             \n",
      " 11  environment_setup_commit  225 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(10)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from SmolCoder.src.llm_wrapper import LLM\n",
    "from SmolCoder.src.prompting_strategy import PromptingStrategy\n",
    "from SmolCoder.src.toolkit import Toolkit\n",
    "from SmolCoder.src.tools.list_methods import ListMethods\n",
    "from SmolCoder.src.tools.list_files import ListFiles\n",
    "from SmolCoder.src.tools.list_classes import ListClasses\n",
    "from SmolCoder.src.tools.finish import Finish\n",
    "from SmolCoder.src.meta_tokenizer import MetaTokenizer\n",
    "\n",
    "from SmolCoder.src.agent import SmolCoder\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import os\n",
    "import validators\n",
    "import shutil\n",
    "\n",
    "df = pd.read_json(\"Evaluation/swe-bench-lite.json\")\n",
    "df.info()\n",
    "\n",
    "list_methods = ListMethods()\n",
    "list_classes = ListClasses()\n",
    "list_files = ListFiles()\n",
    "finish = Finish()\n",
    "\n",
    "toolkit = Toolkit([list_methods, list_classes, list_files, finish])\n",
    "\n",
    "smol = SmolCoder(phase=2, model=LLM(\"llama3.1\", openai=[False, \"None\"], logger=None), codebase_dir= Path(\"test_codebase/\"), logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89207a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUS CODE SNIPPET PHASE: \n",
      "\n",
      "\n",
      "You will be given a description of a `GitHub issue` and it's corresponding codebase and your task is, to solve this issue. First you will be given a tree structure of the codebase, your task is it based on the description of the issue to select relevant files of it for closer inspection. After this you will be provided with a skeleten for each of your slected file, this skeleton will consist out of class and method headers and your task will be to select the classes and methods that are relevant to the described issue. At the end you will be provided with the source code of your selected classes and methos and asked to fix it.\n",
      "--------------------------------------------\n",
      "You will now be given the description of the GitHub Issue: \n",
      "\n",
      "Enable quiet mode/no-verbose in CLI for use in pre-commit hook\n",
      "There seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\n",
      "\n",
      "It would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\n",
      "![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\n",
      "\n",
      "This hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n",
      "\n",
      "--------------------------------------------\n",
      "We now want to identify whether the following code snippet is relevant to the described issue.\n",
      "You will be provided with a source code snippet. You should decide whether it is relevant to the issue.\n",
      "Please respond with either 'YES.' or 'NO.'.\n",
      "Example Answer:\n",
      "YES.--------------------------------------------\n",
      "Here is the code snippet:\n",
      "def _iter_segments(\n",
      "    lexed_elements: List[TemplateElement],\n",
      "    templated_file: TemplatedFile,\n",
      "    add_indents: bool = True,\n",
      ") -> Iterator[RawSegment]:\n",
      "    # An index to track where we've got to in the templated file.\n",
      "    tfs_idx = 0\n",
      "    # We keep a map of previous block locations in case they re-occur.\n",
      "    block_stack = BlockTracker()\n",
      "    templated_file_slices = templated_file.sliced_file\n",
      "\n",
      "    # Now work out source slices, and add in template placeholders.\n",
      "    for idx, element in enumerate(lexed_elements):\n",
      "        # We're working through elements in the rendered file.\n",
      "        # When they enter this code they don't have a position in the source.\n",
      "        # We already have a map of how templated elements map to the source file\n",
      "        # so we work through them to work out what's going on. In theory we can\n",
      "        # step through the two lists in lock step.\n",
      "\n",
      "        # i.e. we worked through the lexed elements, but check off the templated\n",
      "        # file slices as we go.\n",
      "\n",
      "        # Output the slice as we lex.\n",
      "        lexer_logger.debug(\"  %s: %s. [tfs_idx = %s]\", idx, element, tfs_idx)\n",
      "\n",
      "        # All lexed elements, by definition, have a position in the templated\n",
      "        # file. That means we've potentially got zero-length elements we also\n",
      "        # need to consider. We certainly need to consider templated slices\n",
      "        # at tfs_idx. But we should consider some others after that which we\n",
      "        # might also need to consider.\n",
      "\n",
      "        # A lexed element is either a literal in the raw file or the result\n",
      "        # (or part of the result) of a template placeholder. We don't make\n",
      "        # placeholders for any variables which return a non-zero length of\n",
      "        # code. We do add placeholders for others.\n",
      "\n",
      "        # The amount of the current element which has already been consumed.\n",
      "        consumed_element_length = 0\n",
      "        # The position in the source which we still need to yield from.\n",
      "        stashed_source_idx = None\n",
      "\n",
      "        for tfs_idx, tfs in enumerate(templated_file_slices[tfs_idx:], tfs_idx):\n",
      "            lexer_logger.debug(\"      %s: %s\", tfs_idx, tfs)\n",
      "\n",
      "            # Is it a zero slice?\n",
      "            if is_zero_slice(tfs.templated_slice):\n",
      "                next_tfs = (\n",
      "                    templated_file_slices[tfs_idx + 1]\n",
      "                    if tfs_idx + 1 < len(templated_file_slices)\n",
      "                    else None\n",
      "                )\n",
      "                yield from _handle_zero_length_slice(\n",
      "                    tfs, next_tfs, block_stack, templated_file, add_indents\n",
      "                )\n",
      "                continue\n",
      "\n",
      "            if tfs.slice_type == \"literal\":\n",
      "                # There's a literal to deal with here. Yield as much as we can.\n",
      "\n",
      "                # Can we cover this whole lexed element with the current templated\n",
      "                # slice without moving on?\n",
      "                tfs_offset = tfs.source_slice.start - tfs.templated_slice.start\n",
      "                # NOTE: Greater than OR EQUAL, to include the case of it matching\n",
      "                # length exactly.\n",
      "                if element.template_slice.stop <= tfs.templated_slice.stop:\n",
      "                    lexer_logger.debug(\n",
      "                        \"     Consuming whole from literal. Existing Consumed: %s\",\n",
      "                        consumed_element_length,\n",
      "                    )\n",
      "                    # If we have a stashed start use that. Otherwise infer start.\n",
      "                    if stashed_source_idx is not None:\n",
      "                        slice_start = stashed_source_idx\n",
      "                    else:\n",
      "                        slice_start = (\n",
      "                            element.template_slice.start\n",
      "                            + consumed_element_length\n",
      "                            + tfs_offset\n",
      "                        )\n",
      "                    yield element.to_segment(\n",
      "                        pos_marker=PositionMarker(\n",
      "                            slice(\n",
      "                                slice_start,\n",
      "                                element.template_slice.stop + tfs_offset,\n",
      "                            ),\n",
      "                            element.template_slice,\n",
      "                            templated_file,\n",
      "                        ),\n",
      "                        subslice=slice(consumed_element_length, None),\n",
      "                    )\n",
      "\n",
      "                    # If it was an exact match, consume the templated element too.\n",
      "                    if element.template_slice.stop == tfs.templated_slice.stop:\n",
      "                        tfs_idx += 1\n",
      "                    # In any case, we're done with this element. Move on\n",
      "                    break\n",
      "                elif element.template_slice.start == tfs.templated_slice.stop:\n",
      "                    # Did we forget to move on from the last tfs and there's\n",
      "                    # overlap?\n",
      "                    # NOTE: If the rest of the logic works, this should never\n",
      "                    # happen.\n",
      "                    lexer_logger.debug(\"     NOTE: Missed Skip\")  # pragma: no cover\n",
      "                    continue  # pragma: no cover\n",
      "                else:\n",
      "                    # This means that the current lexed element spans across\n",
      "                    # multiple templated file slices.\n",
      "                    lexer_logger.debug(\"     Consuming whole spanning literal\")\n",
      "                    # This almost certainly means there's a templated element\n",
      "                    # in the middle of a whole lexed element.\n",
      "\n",
      "                    # What we do here depends on whether we're allowed to split\n",
      "                    # lexed elements. This is basically only true if it's whitespace.\n",
      "                    # NOTE: We should probably make this configurable on the\n",
      "                    # matcher object, but for now we're going to look for the\n",
      "                    # name of the lexer.\n",
      "                    if element.matcher.name == \"whitespace\":\n",
      "                        # We *can* split it!\n",
      "                        # Consume what we can from this slice and move on.\n",
      "                        lexer_logger.debug(\n",
      "                            \"     Consuming split whitespace from literal. \"\n",
      "                            \"Existing Consumed: %s\",\n",
      "                            consumed_element_length,\n",
      "                        )\n",
      "                        if stashed_source_idx is not None:\n",
      "                            raise NotImplementedError(  # pragma: no cover\n",
      "                                \"Found literal whitespace with stashed idx!\"\n",
      "                            )\n",
      "                        incremental_length = (\n",
      "                            tfs.templated_slice.stop - element.template_slice.start\n",
      "                        )\n",
      "                        yield element.to_segment(\n",
      "                            pos_marker=PositionMarker(\n",
      "                                slice(\n",
      "                                    element.template_slice.start\n",
      "                                    + consumed_element_length\n",
      "                                    + tfs_offset,\n",
      "                                    tfs.templated_slice.stop + tfs_offset,\n",
      "                                ),\n",
      "                                element.template_slice,\n",
      "                                templated_file,\n",
      "                            ),\n",
      "                            # Subdivide the existing segment.\n",
      "                            subslice=offset_slice(\n",
      "                                consumed_element_length,\n",
      "                                incremental_length,\n",
      "                            ),\n",
      "                        )\n",
      "                        consumed_element_length += incremental_length\n",
      "                        continue\n",
      "                    else:\n",
      "                        # We can't split it. We're going to end up yielding a segment\n",
      "                        # which spans multiple slices. Stash the type, and if we haven't\n",
      "                        # set the start yet, stash it too.\n",
      "                        lexer_logger.debug(\"     Spilling over literal slice.\")\n",
      "                        if stashed_source_idx is None:\n",
      "                            stashed_source_idx = (\n",
      "                                element.template_slice.start + tfs_offset\n",
      "                            )\n",
      "                            lexer_logger.debug(\n",
      "                                \"     Stashing a source start. %s\", stashed_source_idx\n",
      "                            )\n",
      "                        continue\n",
      "\n",
      "            elif tfs.slice_type in (\"templated\", \"block_start\", \"escaped\"):\n",
      "                # Found a templated slice. Does it have length in the templated file?\n",
      "                # If it doesn't, then we'll pick it up next.\n",
      "                if not is_zero_slice(tfs.templated_slice):\n",
      "                    # If it's a block_start. Append to the block stack.\n",
      "                    # NOTE: This is rare, but call blocks do occasionally\n",
      "                    # have length (and so don't get picked up by\n",
      "                    # _handle_zero_length_slice)\n",
      "                    if tfs.slice_type == \"block_start\":\n",
      "                        block_stack.enter(tfs.source_slice)\n",
      "\n",
      "                    # Is our current element totally contained in this slice?\n",
      "                    if element.template_slice.stop <= tfs.templated_slice.stop:\n",
      "                        lexer_logger.debug(\"     Contained templated slice.\")\n",
      "                        # Yes it is. Add lexed element with source slices as the whole\n",
      "                        # span of the source slice for the file slice.\n",
      "                        # If we've got an existing stashed source start, use that\n",
      "                        # as the start of the source slice.\n",
      "                        if stashed_source_idx is not None:\n",
      "                            slice_start = stashed_source_idx\n",
      "                        else:\n",
      "                            slice_start = (\n",
      "                                tfs.source_slice.start + consumed_element_length\n",
      "                            )\n",
      "                        yield element.to_segment(\n",
      "                            pos_marker=PositionMarker(\n",
      "                                slice(\n",
      "                                    slice_start,\n",
      "                                    # The end in the source is the end of the templated\n",
      "                                    # slice. We can't subdivide any better.\n",
      "                                    tfs.source_slice.stop,\n",
      "                                ),\n",
      "                                element.template_slice,\n",
      "                                templated_file,\n",
      "                            ),\n",
      "                            subslice=slice(consumed_element_length, None),\n",
      "                        )\n",
      "\n",
      "                        # If it was an exact match, consume the templated element too.\n",
      "                        if element.template_slice.stop == tfs.templated_slice.stop:\n",
      "                            tfs_idx += 1\n",
      "                        # Carry on to the next lexed element\n",
      "                        break\n",
      "                    # We've got an element which extends beyond this templated slice.\n",
      "                    # This means that a _single_ lexed element claims both some\n",
      "                    # templated elements and some non-templated elements. That could\n",
      "                    # include all kinds of things (and from here we don't know what\n",
      "                    # else is yet to come, comments, blocks, literals etc...).\n",
      "\n",
      "                    # In the `literal` version of this code we would consider\n",
      "                    # splitting the literal element here, but in the templated\n",
      "                    # side we don't. That's because the way that templated tokens\n",
      "                    # are lexed, means that they should arrive \"pre-split\".\n",
      "                    else:\n",
      "                        # Stash the source idx for later when we do make a segment.\n",
      "                        lexer_logger.debug(\"     Spilling over templated slice.\")\n",
      "                        if stashed_source_idx is None:\n",
      "                            stashed_source_idx = tfs.source_slice.start\n",
      "                            lexer_logger.debug(\n",
      "                                \"     Stashing a source start as lexed element spans \"\n",
      "                                \"over the end of a template slice. %s\",\n",
      "                                stashed_source_idx,\n",
      "                            )\n",
      "                        # Move on to the next template slice\n",
      "                        continue\n",
      "\n",
      "            raise NotImplementedError(\n",
      "                f\"Unable to process slice: {tfs}\"\n",
      "            )  # pragma: no cover\n",
      "\n",
      "    # If templated elements are left, yield them.\n",
      "    # We can assume they're all zero length if we're here.\n",
      "    for tfs_idx, tfs in enumerate(templated_file_slices[tfs_idx:], tfs_idx):\n",
      "        next_tfs = (\n",
      "            templated_file_slices[tfs_idx + 1]\n",
      "            if tfs_idx + 1 < len(templated_file_slices)\n",
      "            else None\n",
      "        )\n",
      "        yield from _handle_zero_length_slice(\n",
      "            tfs, next_tfs, block_stack, templated_file, add_indents\n",
      "        )\n",
      "--------------------------------------------\n",
      "Is this code snippet relevant to the issue? Please respond with 'YES.' or 'NO.'.\n",
      "\n",
      "NO.\n",
      "You will be given a description of a `GitHub issue` and it's corresponding codebase and your task is, to solve this issue. First you will be given a tree structure of the codebase, your task is it based on the description of the issue to select relevant files of it for closer inspection. After this you will be provided with a skeleten for each of your slected file, this skeleton will consist out of class and method headers and your task will be to select the classes and methods that are relevant to the described issue. At the end you will be provided with the source code of your selected classes and methos and asked to fix it.\n",
      "--------------------------------------------\n",
      "You will now be given the description of the GitHub Issue: \n",
      "\n",
      "Enable quiet mode/no-verbose in CLI for use in pre-commit hook\n",
      "There seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\n",
      "\n",
      "It would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\n",
      "![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\n",
      "\n",
      "This hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n",
      "\n",
      "--------------------------------------------\n",
      "We now want to identify whether the following code snippet is relevant to the described issue.\n",
      "You will be provided with a source code snippet. You should decide whether it is relevant to the issue.\n",
      "Please respond with either 'YES.' or 'NO.'.\n",
      "Example Answer:\n",
      "YES.--------------------------------------------\n",
      "Here is the code snippet:\n",
      "def _handle_zero_length_slice(\n",
      "    tfs: TemplatedFileSlice,\n",
      "    next_tfs: Optional[TemplatedFileSlice],\n",
      "    block_stack: BlockTracker,\n",
      "    templated_file: TemplatedFile,\n",
      "    add_indents: bool,\n",
      ") -> Iterator[MetaSegment]:\n",
      "    \"\"\"Generate placeholders and loop segments from a zero length slice.\n",
      "\n",
      "    This method checks for:\n",
      "    1. Backward jumps (inserting :obj:`TemplateLoop`).\n",
      "    2. Forward jumps (inserting :obj:`TemplateSegment`).\n",
      "    3. Blocks (inserting :obj:`TemplateSegment`).\n",
      "    4. Unrendered template elements(inserting :obj:`TemplateSegment`).\n",
      "\n",
      "    For blocks and loops, :obj:`Indent` and :obj:`Dedent` segments are\n",
      "    yielded around them as appropriate.\n",
      "\n",
      "    NOTE: block_stack is _mutated_ by this method.\n",
      "    \"\"\"\n",
      "    assert is_zero_slice(tfs.templated_slice)\n",
      "    # First check for jumps. Backward initially, because in the backward\n",
      "    # case we don't render the element we find first.\n",
      "    # That requires being able to look past to the next element.\n",
      "    if tfs.slice_type.startswith(\"block\") and next_tfs:\n",
      "        # Look for potential backward jump\n",
      "        if next_tfs.source_slice.start < tfs.source_slice.start:\n",
      "            lexer_logger.debug(\"      Backward jump detected. Inserting Loop Marker\")\n",
      "            # If we're here remember we're on the tfs which is the block end\n",
      "            # i.e. not the thing we want to render.\n",
      "            pos_marker = PositionMarker.from_point(\n",
      "                tfs.source_slice.start,\n",
      "                tfs.templated_slice.start,\n",
      "                templated_file,\n",
      "            )\n",
      "            if add_indents:\n",
      "                yield Dedent(\n",
      "                    is_template=True,\n",
      "                    pos_marker=pos_marker,\n",
      "                )\n",
      "\n",
      "            yield TemplateLoop(pos_marker=pos_marker, block_uuid=block_stack.top())\n",
      "\n",
      "            if add_indents:\n",
      "                yield Indent(\n",
      "                    is_template=True,\n",
      "                    pos_marker=pos_marker,\n",
      "                )\n",
      "            # Move on to the next templated slice. Don't render this directly.\n",
      "            return\n",
      "\n",
      "    # Then handle blocks (which aren't jumps backward)\n",
      "    if tfs.slice_type.startswith(\"block\"):\n",
      "        # It's a block. Yield a placeholder with potential indents.\n",
      "\n",
      "        # Update block stack or add indents\n",
      "        if tfs.slice_type == \"block_start\":\n",
      "            block_stack.enter(tfs.source_slice)\n",
      "        elif add_indents and tfs.slice_type in (\"block_end\", \"block_mid\"):\n",
      "            yield Dedent(\n",
      "                is_template=True,\n",
      "                pos_marker=PositionMarker.from_point(\n",
      "                    tfs.source_slice.start,\n",
      "                    tfs.templated_slice.start,\n",
      "                    templated_file,\n",
      "                ),\n",
      "                # NOTE: We mark the dedent with the block uuid too.\n",
      "                block_uuid=block_stack.top(),\n",
      "            )\n",
      "\n",
      "        yield TemplateSegment.from_slice(\n",
      "            tfs.source_slice,\n",
      "            tfs.templated_slice,\n",
      "            block_type=tfs.slice_type,\n",
      "            templated_file=templated_file,\n",
      "            block_uuid=block_stack.top(),\n",
      "        )\n",
      "\n",
      "        # Update block stack or add indents\n",
      "        if tfs.slice_type == \"block_end\":\n",
      "            block_stack.exit()\n",
      "        elif add_indents and tfs.slice_type in (\"block_start\", \"block_mid\"):\n",
      "            yield Indent(\n",
      "                is_template=True,\n",
      "                pos_marker=PositionMarker.from_point(\n",
      "                    tfs.source_slice.stop,\n",
      "                    tfs.templated_slice.stop,\n",
      "                    templated_file,\n",
      "                ),\n",
      "                # NOTE: We mark the indent with the block uuid too.\n",
      "                block_uuid=block_stack.top(),\n",
      "            )\n",
      "\n",
      "        # Before we move on, we might have a _forward_ jump to the next\n",
      "        # element. That element can handle itself, but we'll add a\n",
      "        # placeholder for it here before we move on.\n",
      "        if next_tfs and next_tfs.source_slice.start > tfs.source_slice.stop:\n",
      "            # We do so extract the string.\n",
      "            placeholder_str = templated_file.source_str[\n",
      "                tfs.source_slice.stop : next_tfs.source_slice.start\n",
      "            ]\n",
      "            # Trim it if it's too long to show.\n",
      "            if len(placeholder_str) >= 20:\n",
      "                placeholder_str = (\n",
      "                    f\"... [{len(placeholder_str)} unused template \" \"characters] ...\"\n",
      "                )\n",
      "            lexer_logger.debug(\"      Forward jump detected. Inserting placeholder\")\n",
      "            yield TemplateSegment(\n",
      "                pos_marker=PositionMarker(\n",
      "                    slice(tfs.source_slice.stop, next_tfs.source_slice.start),\n",
      "                    # Zero slice in the template.\n",
      "                    tfs.templated_slice,\n",
      "                    templated_file,\n",
      "                ),\n",
      "                source_str=placeholder_str,\n",
      "                block_type=\"skipped_source\",\n",
      "            )\n",
      "\n",
      "        # Move on\n",
      "        return\n",
      "\n",
      "    # Always return the slice, even if the source slice was also zero length.  Some\n",
      "    # templaters might want to pass through totally zero length slices as a way of\n",
      "    # marking locations in the middle of templated output.\n",
      "    yield TemplateSegment.from_slice(\n",
      "        tfs.source_slice,\n",
      "        tfs.templated_slice,\n",
      "        tfs.slice_type,\n",
      "        templated_file,\n",
      "    )\n",
      "--------------------------------------------\n",
      "Is this code snippet relevant to the issue? Please respond with 'YES.' or 'NO.'.\n",
      "\n",
      "Note: I have not read the entire source code of SQLFluff, and therefore cannot answer based on the entire project.\n",
      " \n",
      "\n",
      "I will try again.\n",
      "Failed to obtain a valid response from the LLM.\n",
      "You will be given a description of a `GitHub issue` and it's corresponding codebase and your task is, to solve this issue. First you will be given a tree structure of the codebase, your task is it based on the description of the issue to select relevant files of it for closer inspection. After this you will be provided with a skeleten for each of your slected file, this skeleton will consist out of class and method headers and your task will be to select the classes and methods that are relevant to the described issue. At the end you will be provided with the source code of your selected classes and methos and asked to fix it.\n",
      "--------------------------------------------\n",
      "You will now be given the description of the GitHub Issue: \n",
      "\n",
      "Enable quiet mode/no-verbose in CLI for use in pre-commit hook\n",
      "There seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\n",
      "\n",
      "It would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\n",
      "![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\n",
      "\n",
      "This hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n",
      "\n",
      "--------------------------------------------\n",
      "We now want to identify whether the following code snippet is relevant to the described issue.\n",
      "You will be provided with a source code snippet. You should decide whether it is relevant to the issue.\n",
      "Please respond with either 'YES.' or 'NO.'.\n",
      "Example Answer:\n",
      "YES.--------------------------------------------\n",
      "Here is the code snippet:\n",
      "class BlockTracker:\n",
      "    \"\"\"This is an object for keeping track of templating blocks.\n",
      "\n",
      "    Using the .enter() and .exit() methods on opening and closing\n",
      "    blocks, we can match up tags of the same level so that later\n",
      "    it's easier to treat them the same way in the linting engine.\n",
      "\n",
      "    In case looping means that we encounter the same block more\n",
      "    than once, we use cache uuids against their source location\n",
      "    so that if we try to re-enter the block again, it will get\n",
      "    the same uuid on the second pass.\n",
      "    \"\"\"\n",
      "\n",
      "    _stack: List[UUID] = []\n",
      "    _map: Dict[Tuple[int, int], UUID] = {}\n",
      "\n",
      "    def enter(self, src_slice: slice) -> None:\n",
      "        \"\"\"Add a block to the stack.\"\"\"\n",
      "        key = to_tuple(src_slice)\n",
      "        uuid = self._map.get(key, None)\n",
      "\n",
      "        if not uuid:\n",
      "            uuid = uuid4()\n",
      "            self._map[key] = uuid\n",
      "            lexer_logger.debug(\n",
      "                \"        Entering block stack @ %s: %s (fresh)\",\n",
      "                src_slice,\n",
      "                uuid,\n",
      "            )\n",
      "        else:\n",
      "            lexer_logger.debug(\n",
      "                \"        Entering block stack @ %s: %s (cached)\",\n",
      "                src_slice,\n",
      "                uuid,\n",
      "            )\n",
      "\n",
      "        self._stack.append(uuid)\n",
      "\n",
      "    def exit(self) -> None:\n",
      "        \"\"\"Pop a block from the stack.\"\"\"\n",
      "        uuid = self._stack.pop()\n",
      "        lexer_logger.debug(\n",
      "            \"        Exiting block stack: %s\",\n",
      "            uuid,\n",
      "        )\n",
      "\n",
      "    def top(self) -> UUID:\n",
      "        \"\"\"Get the uuid on top of the stack.\"\"\"\n",
      "        return self._stack[-1]\n",
      "--------------------------------------------\n",
      "Is this code snippet relevant to the issue? Please respond with 'YES.' or 'NO.'.\n",
      "\n",
      "Note that I made an edit to your response format.\n",
      " (If you're unsure, NO.\n",
      "Failed to obtain a valid response from the LLM.\n",
      "You will be given a description of a `GitHub issue` and it's corresponding codebase and your task is, to solve this issue. First you will be given a tree structure of the codebase, your task is it based on the description of the issue to select relevant files of it for closer inspection. After this you will be provided with a skeleten for each of your slected file, this skeleton will consist out of class and method headers and your task will be to select the classes and methods that are relevant to the described issue. At the end you will be provided with the source code of your selected classes and methos and asked to fix it.\n",
      "--------------------------------------------\n",
      "You will now be given the description of the GitHub Issue: \n",
      "\n",
      "Enable quiet mode/no-verbose in CLI for use in pre-commit hook\n",
      "There seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\n",
      "\n",
      "It would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\n",
      "![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\n",
      "\n",
      "This hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n",
      "\n",
      "--------------------------------------------\n",
      "We now want to identify whether the following code snippet is relevant to the described issue.\n",
      "You will be provided with a source code snippet. You should decide whether it is relevant to the issue.\n",
      "Please respond with either 'YES.' or 'NO.'.\n",
      "Example Answer:\n",
      "YES.--------------------------------------------\n",
      "Here is the code snippet:\n",
      "class Lexer:\n",
      "    \"\"\"The Lexer class actually does the lexing step.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: Optional[FluffConfig] = None,\n",
      "        last_resort_lexer: Optional[StringLexer] = None,\n",
      "        dialect: Optional[str] = None,\n",
      "    ):\n",
      "        # Allow optional config and dialect\n",
      "        self.config = FluffConfig.from_kwargs(config=config, dialect=dialect)\n",
      "        # Store the matchers\n",
      "        self.lexer_matchers = self.config.get(\"dialect_obj\").get_lexer_matchers()\n",
      "\n",
      "        self.last_resort_lexer = last_resort_lexer or RegexLexer(\n",
      "            \"<unlexable>\",\n",
      "            r\"[^\\t\\n\\ ]*\",\n",
      "            UnlexableSegment,\n",
      "        )\n",
      "\n",
      "    def lex(\n",
      "        self, raw: Union[str, TemplatedFile]\n",
      "    ) -> Tuple[Tuple[BaseSegment, ...], List[SQLLexError]]:\n",
      "        \"\"\"Take a string or TemplatedFile and return segments.\n",
      "\n",
      "        If we fail to match the *whole* string, then we must have\n",
      "        found something that we cannot lex. If that happens we should\n",
      "        package it up as unlexable and keep track of the exceptions.\n",
      "        \"\"\"\n",
      "        # Make sure we've got a string buffer and a template\n",
      "        # regardless of what was passed in.\n",
      "        if isinstance(raw, str):\n",
      "            template = TemplatedFile.from_string(raw)\n",
      "            str_buff = raw\n",
      "        else:\n",
      "            template = raw\n",
      "            str_buff = str(template)\n",
      "\n",
      "        # Lex the string to get a tuple of LexedElement\n",
      "        element_buffer: List[LexedElement] = []\n",
      "        while True:\n",
      "            res = self.lex_match(str_buff, self.lexer_matchers)\n",
      "            element_buffer += res.elements\n",
      "            if res.forward_string:\n",
      "                resort_res = self.last_resort_lexer.match(res.forward_string)\n",
      "                if not resort_res:  # pragma: no cover\n",
      "                    # If we STILL can't match, then just panic out.\n",
      "                    raise SQLLexError(\n",
      "                        \"Fatal. Unable to lex characters: {0!r}\".format(\n",
      "                            res.forward_string[:10] + \"...\"\n",
      "                            if len(res.forward_string) > 9\n",
      "                            else res.forward_string\n",
      "                        )\n",
      "                    )\n",
      "                str_buff = resort_res.forward_string\n",
      "                element_buffer += resort_res.elements\n",
      "            else:  # pragma: no cover TODO?\n",
      "                break\n",
      "\n",
      "        # Map tuple LexedElement to list of TemplateElement.\n",
      "        # This adds the template_slice to the object.\n",
      "        templated_buffer = self.map_template_slices(element_buffer, template)\n",
      "\n",
      "        # Turn lexed elements into segments.\n",
      "        segments: Tuple[RawSegment, ...] = self.elements_to_segments(\n",
      "            templated_buffer, template\n",
      "        )\n",
      "\n",
      "        # Generate any violations\n",
      "        violations: List[SQLLexError] = self.violations_from_segments(segments)\n",
      "\n",
      "        return segments, violations\n",
      "\n",
      "    def elements_to_segments(\n",
      "        self, elements: List[TemplateElement], templated_file: TemplatedFile\n",
      "    ) -> Tuple[RawSegment, ...]:\n",
      "        \"\"\"Convert a tuple of lexed elements into a tuple of segments.\"\"\"\n",
      "        lexer_logger.info(\"Elements to Segments.\")\n",
      "        add_indents = self.config.get(\"template_blocks_indent\", \"indentation\")\n",
      "        # Delegate to _iter_segments\n",
      "        segment_buffer: List[RawSegment] = list(\n",
      "            _iter_segments(elements, templated_file, add_indents)\n",
      "        )\n",
      "\n",
      "        # Add an end of file marker\n",
      "        segment_buffer.append(\n",
      "            EndOfFile(\n",
      "                pos_marker=(\n",
      "                    segment_buffer[-1].pos_marker.end_point_marker()\n",
      "                    if segment_buffer\n",
      "                    else PositionMarker.from_point(0, 0, templated_file)\n",
      "                )\n",
      "            )\n",
      "        )\n",
      "        # Convert to tuple before return\n",
      "        return tuple(segment_buffer)\n",
      "\n",
      "    @staticmethod\n",
      "    def violations_from_segments(segments: Tuple[RawSegment, ...]) -> List[SQLLexError]:\n",
      "        \"\"\"Generate any lexing errors for any unlexables.\"\"\"\n",
      "        violations = []\n",
      "        for segment in segments:\n",
      "            if segment.is_type(\"unlexable\"):\n",
      "                violations.append(\n",
      "                    SQLLexError(\n",
      "                        \"Unable to lex characters: {!r}\".format(\n",
      "                            segment.raw[:10] + \"...\"\n",
      "                            if len(segment.raw) > 9\n",
      "                            else segment.raw\n",
      "                        ),\n",
      "                        pos=segment.pos_marker,\n",
      "                    )\n",
      "                )\n",
      "        return violations\n",
      "\n",
      "    @staticmethod\n",
      "    def lex_match(forward_string: str, lexer_matchers: List[StringLexer]) -> LexMatch:\n",
      "        \"\"\"Iteratively match strings using the selection of submatchers.\"\"\"\n",
      "        elem_buff: List[LexedElement] = []\n",
      "        while True:\n",
      "            if len(forward_string) == 0:\n",
      "                return LexMatch(forward_string, elem_buff)\n",
      "            for matcher in lexer_matchers:\n",
      "                res = matcher.match(forward_string)\n",
      "                if res.elements:\n",
      "                    # If we have new segments then whoop!\n",
      "                    elem_buff += res.elements\n",
      "                    forward_string = res.forward_string\n",
      "                    # Cycle back around again and start with the top\n",
      "                    # matcher again.\n",
      "                    break\n",
      "            else:\n",
      "                # We've got so far, but now can't match. Return\n",
      "                return LexMatch(forward_string, elem_buff)\n",
      "\n",
      "    @staticmethod\n",
      "    def map_template_slices(\n",
      "        elements: List[LexedElement], template: TemplatedFile\n",
      "    ) -> List[TemplateElement]:\n",
      "        \"\"\"Create a tuple of TemplateElement from a tuple of LexedElement.\n",
      "\n",
      "        This adds slices in the templated file to the original lexed\n",
      "        elements. We'll need this to work out the position in the source\n",
      "        file.\n",
      "        \"\"\"\n",
      "        idx = 0\n",
      "        templated_buff: List[TemplateElement] = []\n",
      "        for element in elements:\n",
      "            template_slice = offset_slice(idx, len(element.raw))\n",
      "            idx += len(element.raw)\n",
      "            templated_buff.append(TemplateElement.from_element(element, template_slice))\n",
      "            if (\n",
      "                template.templated_str[template_slice] != element.raw\n",
      "            ):  # pragma: no cover\n",
      "                raise ValueError(\n",
      "                    \"Template and lexed elements do not match. This should never \"\n",
      "                    f\"happen {element.raw!r} != \"\n",
      "                    f\"{template.templated_str[template_slice]!r}\"\n",
      "                )\n",
      "        return templated_buff\n",
      "--------------------------------------------\n",
      "Is this code snippet relevant to the issue? Please respond with 'YES.' or 'NO.'.\n",
      "\n",
      "Note: Your answer will be compared to the actual relevance of the code.\n",
      " I'll assume it's 'NO.\n",
      "Failed to obtain a valid response from the LLM.\n",
      "No relevant classes/functions were found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " [{'file_path': './repos/sqlfluff/src/sqlfluff/core/config.py',\n",
       "   'selected_functions': [],\n",
       "   'selected_classes': ['ConfigLoader', 'FluffConfig']},\n",
       "  {'file_path': './repos/sqlfluff/src/sqlfluff/core/linter/linted_file.py',\n",
       "   'selected_functions': [],\n",
       "   'selected_classes': ['LintedFile']},\n",
       "  {'file_path': './repos/sqlfluff/src/sqlfluff/core/parser/lexer.py',\n",
       "   'selected_functions': [],\n",
       "   'selected_classes': ['TemplateElement',\n",
       "    'LexMatch',\n",
       "    'StringLexer',\n",
       "    'RegexLexer',\n",
       "    'Lexer']},\n",
       "  {'file_path': './repos/sqlfluff/src/sqlfluff/core/parser/matchable.py',\n",
       "   'selected_functions': ['match'],\n",
       "   'selected_classes': ['Matchable']}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smol(df.iloc[0][\"problem_statement\"], start_cwd=\"./repos/sqlfluff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea639ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
