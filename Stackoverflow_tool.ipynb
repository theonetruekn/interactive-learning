{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413a453b-4213-476f-b08a-553f2f765099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86bcebc6-d001-4f80-ba05-1a9f3be54dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, Any, Optional, List\n",
    "from markdownify import markdownify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7a48c86-4879-4f5e-b999-bebdcae0503e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _search_stackoverflow_for_relevant_questions(tag: str, query: str, num_results: int = 5) -> Optional[List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Search Stack Overflow for questions based on a specified tag and query.\n",
    "\n",
    "    Args:\n",
    "        tag (str): The tag to search for.\n",
    "        query (str): The query to search for in the title of questions.\n",
    "        num_results (int, optional): The number of results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        Optional[List[Tuple[str, str]]]: A list of tuples containing the question body and accepted answer body,\n",
    "                                           or None if an error occurs during the request.\n",
    "    \"\"\"\n",
    "    # Stack Exchange API endpoint for searching questions\n",
    "    url = \"https://api.stackexchange.com/2.3/search\"\n",
    "    \n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"order\": \"desc\",\n",
    "        \"sort\": \"relevance\",\n",
    "        \"tagged\": tag,  # specify the tag\n",
    "        \"intitle\": query,  # specify the query in title\n",
    "        \"site\": \"stackoverflow\",\n",
    "        \"pagesize\": num_results,  # specify the number of results\n",
    "        \"answers\": 1,  # get questions with at least 1 answer\n",
    "        \"accepted\": True,  # get questions with accepted answers\n",
    "        \"filter\": \"withbody\"  # include answer body in response\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        data = response.json()\n",
    "        \n",
    "        results = []\n",
    "        # Extract relevant information for each question\n",
    "        for item in data.get(\"items\", []):\n",
    "            question_body = markdownify(item[\"body\"], strip=[\"a\", \"blockquote\"]).rstrip(\"\\n\")\n",
    "            \n",
    "            # Fetch accepted answer if available\n",
    "            if \"accepted_answer_id\" in item:\n",
    "                answer_id = item[\"accepted_answer_id\"]\n",
    "                answer_url = f\"https://api.stackexchange.com/2.3/answers/{answer_id}\"\n",
    "                answer_params = {\n",
    "                    \"site\": \"stackoverflow\",\n",
    "                    \"filter\": \"withbody\"  # include answer body in response\n",
    "                }\n",
    "                answer_response = requests.get(answer_url, params=answer_params)\n",
    "                answer_data = answer_response.json()\n",
    "                if \"items\" in answer_data and len(answer_data[\"items\"]) > 0:\n",
    "                    # Check if \"body\" exists and is not empty in the answer data\n",
    "                    if \"body\" in answer_data[\"items\"][0] and answer_data[\"items\"][0][\"body\"].strip():\n",
    "                        answer_body = markdownify(answer_data[\"items\"][0][\"body\"], strip=[\"a\", \"blockquote\"]).rstrip(\"\\n\")\n",
    "                        results.append((question_body, answer_body))\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "346495c8-1ce7-4fd4-93a4-d142c8b293a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_stackoferflow_questions(input_questions: List[Tuple[str, str]]) -> str:\n",
    "    formatted_result = \"\"\n",
    "    for result in input_questions:\n",
    "        formatted_result += f\"---------------\\nQuestion:\\n{result[0]}\\n--------------- \\nAccepted Answer:\\n{result[1]}\"\n",
    "\n",
    "    return formatted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18f6ec48-b4ee-47f5-a50a-b103789f08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "results = _search_stackoverflow_for_relevant_questions(\"python\", \"pytorch\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6e38a96-05fe-4db3-8052-d5c5ec5c8a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Question:\n",
      "How do I check if PyTorch is using the GPU? The `nvidia-smi` command can detect GPU activity, but I want to check it directly from inside a Python script.\n",
      "--------------- \n",
      "Accepted Answer:\n",
      "These functions should help:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      ">>> import torch\n",
      "\n",
      ">>> torch.cuda.is_available()\n",
      "True\n",
      "\n",
      ">>> torch.cuda.device_count()\n",
      "1\n",
      "\n",
      ">>> torch.cuda.current_device()\n",
      "0\n",
      "\n",
      ">>> torch.cuda.device(0)\n",
      "<torch.cuda.device at 0x7efce0b03be0>\n",
      "\n",
      ">>> torch.cuda.get_device_name(0)\n",
      "'GeForce GTX 950M'\n",
      "\n",
      "```\n",
      "\n",
      "This tells us:\n",
      "\n",
      "\n",
      "* CUDA is available and can be used by one device.\n",
      "* `Device 0` refers to the GPU `GeForce GTX 950M`, and it is currently chosen by PyTorch.---------------\n",
      "Question:\n",
      "How do I save a trained model in PyTorch? I have read that:\n",
      "\n",
      "\n",
      "1. `torch.save()`/`torch.load()` is for saving/loading a serializable object.\n",
      "2. `model.state_dict()`/`model.load_state_dict()` is for saving/loading model state.\n",
      "--------------- \n",
      "Accepted Answer:\n",
      "Found this page on their github repo:\n",
      "\n",
      "\n",
      "\n",
      "#### Recommended approach for saving a model\n",
      "\n",
      "\n",
      "There are two main approaches for serializing and restoring a model.\n",
      "\n",
      "\n",
      "The first (recommended) saves and loads only the model parameters:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "torch.save(the_model.state_dict(), PATH)\n",
      "\n",
      "```\n",
      "\n",
      "Then later:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "the_model = TheModelClass(*args, **kwargs)\n",
      "the_model.load_state_dict(torch.load(PATH))\n",
      "\n",
      "```\n",
      "\n",
      "The second saves and loads the entire model:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "torch.save(the_model, PATH)\n",
      "\n",
      "```\n",
      "\n",
      "Then later:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "the_model = torch.load(PATH)\n",
      "\n",
      "```\n",
      "\n",
      "However in this case, the serialized data is bound to the specific classes and the exact directory structure used, so it can break in various ways when used in other projects, or after some serious refactors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "See also: Save and Load the Model section from the official PyTorch tutorials.---------------\n",
      "Question:\n",
      "What does `.view()` do to a tensor `x`? What do negative values mean?\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "x = x.view(-1, 16 * 5 * 5)\n",
      "\n",
      "```\n",
      "--------------- \n",
      "Accepted Answer:\n",
      "`view()` reshapes the tensor without copying memory, similar to numpy's `reshape()`.\n",
      "\n",
      "\n",
      "Given a tensor `a` with 16 elements:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "import torch\n",
      "a = torch.range(1, 16)\n",
      "\n",
      "```\n",
      "\n",
      "To reshape this tensor to make it a `4 x 4` tensor, use:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "a = a.view(4, 4)\n",
      "\n",
      "```\n",
      "\n",
      "Now `a` will be a `4 x 4` tensor. *Note that after the reshape the total number of elements need to remain the same. Reshaping the tensor `a` to a `3 x 5` tensor would not be appropriate.*\n",
      "\n",
      "\n",
      "### What is the meaning of parameter -1?\n",
      "\n",
      "\n",
      "If there is any situation that you don't know how many rows you want but are sure of the number of columns, then you can specify this with a -1. (*Note that you can extend this to tensors with more dimensions. Only one of the axis value can be -1*). This is a way of telling the library: \"give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen\".\n",
      "\n",
      "\n",
      "This can be seen in this model definition code. After the line `x = self.pool(F.relu(self.conv2(x)))` in the forward function, you will have a 16 depth feature map. You have to flatten this to give it to the fully connected layer. So you tell PyTorch to reshape the tensor you obtained to have specific number of columns and tell it to decide the number of rows by itself.---------------\n",
      "Question:\n",
      "Why does `zero_grad()` need to be called during training?\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "|  zero_grad(self)\n",
      "|      Sets gradients of all model parameters to zero.\n",
      "\n",
      "```\n",
      "--------------- \n",
      "Accepted Answer:\n",
      "In `PyTorch`, for every mini-batch during the *training* phase, we typically want to explicitly set the gradients to zero before starting to do backpropagation (i.e., updating the ***W**eights* and ***b**iases*) because PyTorch *accumulates the gradients* on subsequent backward passes. This accumulating behavior is convenient while training RNNs or when we want to compute the gradient of the loss summed over multiple *mini-batches*. So, the default action has been set to accumulate (i.e. sum) the gradients on every `loss.backward()` call.\n",
      "\n",
      "\n",
      "Because of this, when you start your training loop, ideally you should `zero out the gradients` so that you do the parameter update correctly. Otherwise, the gradient would be a combination of the old gradient, which you have already used to update your model parameters and the newly-computed gradient. It would therefore point in some other direction than the intended direction towards the *minimum* (or *maximum*, in case of maximization objectives).\n",
      "\n",
      "\n",
      "Here is a simple example:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "import torch\n",
      "from torch.autograd import Variable\n",
      "import torch.optim as optim\n",
      "\n",
      "def linear_model(x, W, b):\n",
      "    return torch.matmul(x, W) + b\n",
      "\n",
      "data, targets = ...\n",
      "\n",
      "W = Variable(torch.randn(4, 3), requires_grad=True)\n",
      "b = Variable(torch.randn(3), requires_grad=True)\n",
      "\n",
      "optimizer = optim.Adam([W, b])\n",
      "\n",
      "for sample, target in zip(data, targets):\n",
      "    # clear out the gradients of all Variables \n",
      "    # in this optimizer (i.e. W, b)\n",
      "    optimizer.zero_grad()\n",
      "    output = linear_model(sample, W, b)\n",
      "    loss = (output - target) ** 2\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Alternatively, if you're doing a *vanilla gradient descent*, then:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "W = Variable(torch.randn(4, 3), requires_grad=True)\n",
      "b = Variable(torch.randn(3), requires_grad=True)\n",
      "\n",
      "for sample, target in zip(data, targets):\n",
      "    # clear out the gradients of Variables \n",
      "    # (i.e. W, b)\n",
      "    W.grad.data.zero_()\n",
      "    b.grad.data.zero_()\n",
      "\n",
      "    output = linear_model(sample, W, b)\n",
      "    loss = (output - target) ** 2\n",
      "    loss.backward()\n",
      "\n",
      "    W -= learning_rate * W.grad.data\n",
      "    b -= learning_rate * b.grad.data\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "**Note**:\n",
      "\n",
      "\n",
      "* The *accumulation* (i.e., *sum*) of gradients happens when `.backward()` is called on the `loss` tensor.\n",
      "* As of v1.7.0, Pytorch offers the option to reset the gradients to `None` `optimizer.zero_grad(set_to_none=True)` instead of filling them with a tensor of zeroes. The docs claim that this setting reduces memory requirements and slightly improves performance, but might be error-prone if not handled carefully.---------------\n",
      "Question:\n",
      "How do I print the summary of a model in PyTorch like what `model.summary()` does in Keras:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "Model Summary:\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1, 15, 27)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 8, 15, 27)     872         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 8, 7, 27)      0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1512)          0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             1513        flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,385\n",
      "Trainable params: 2,385\n",
      "Non-trainable params: 0\n",
      "\n",
      "```\n",
      "--------------- \n",
      "Accepted Answer:\n",
      "While you will not get as detailed information about the model as in Keras' model.summary, simply printing the model will give you some idea about the different layers involved and their specifications.\n",
      "\n",
      "\n",
      "For instance:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "from torchvision import models\n",
      "model = models.vgg16()\n",
      "print(model)\n",
      "\n",
      "```\n",
      "\n",
      "The output in this case would be something as follows:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "VGG (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU (inplace)\n",
      "    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU (inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU (inplace)\n",
      "    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU (inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU (inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU (inplace)\n",
      "    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU (inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU (inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU (inplace)\n",
      "    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU (inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU (inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU (inplace)\n",
      "    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Linear (25088 -> 4096)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): Dropout (p = 0.5)\n",
      "    (4): Linear (4096 -> 4096)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): Linear (4096 -> 1000)\n",
      "  )\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "Now you could, as mentioned by Kashyap, use the `state_dict` method to get the weights of the different layers. But using this listing of the layers would perhaps provide more direction is creating a helper function to get that Keras like model summary!\n"
     ]
    }
   ],
   "source": [
    "print(format_stackoferflow_questions(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0783955e-aabe-47b5-b2af-65071bbd2b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Question:\n",
      "How do I check if PyTorch is using the GPU? The `nvidia-smi` command can detect GPU activity, but I want to check it directly from inside a Python script.\n",
      "--------------- \n",
      "Accepted Answer:\n",
      "These functions should help:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      ">>> import torch\n",
      "\n",
      ">>> torch.cuda.is_available()\n",
      "True\n",
      "\n",
      ">>> torch.cuda.device_count()\n",
      "1\n",
      "\n",
      ">>> torch.cuda.current_device()\n",
      "0\n",
      "\n",
      ">>> torch.cuda.device(0)\n",
      "<torch.cuda.device at 0x7efce0b03be0>\n",
      "\n",
      ">>> torch.cuda.get_device_name(0)\n",
      "'GeForce GTX 950M'\n",
      "\n",
      "```\n",
      "\n",
      "This tells us:\n",
      "\n",
      "\n",
      "* CUDA is available and can be used by one device.\n",
      "* `Device 0` refers to the GPU `GeForce GTX 950M`, and it is currently chosen by PyTorch.\n",
      "---------------\n",
      "Question:\n",
      "How do I save a trained model in PyTorch? I have read that:\n",
      "\n",
      "\n",
      "1. `torch.save()`/`torch.load()` is for saving/loading a serializable object.\n",
      "2. `model.state_dict()`/`model.load_state_dict()` is for saving/loading model state.\n",
      "--------------- \n",
      "Accepted Answer:\n",
      "Found this page on their github repo:\n",
      "\n",
      "\n",
      "\n",
      "#### Recommended approach for saving a model\n",
      "\n",
      "\n",
      "There are two main approaches for serializing and restoring a model.\n",
      "\n",
      "\n",
      "The first (recommended) saves and loads only the model parameters:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "torch.save(the_model.state_dict(), PATH)\n",
      "\n",
      "```\n",
      "\n",
      "Then later:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "the_model = TheModelClass(*args, **kwargs)\n",
      "the_model.load_state_dict(torch.load(PATH))\n",
      "\n",
      "```\n",
      "\n",
      "The second saves and loads the entire model:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "torch.save(the_model, PATH)\n",
      "\n",
      "```\n",
      "\n",
      "Then later:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "the_model = torch.load(PATH)\n",
      "\n",
      "```\n",
      "\n",
      "However in this case, the serialized data is bound to the specific classes and the exact directory structure used, so it can break in various ways when used in other projects, or after some serious refactors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "See also: Save and Load the Model section from the official PyTorch tutorials.\n"
     ]
    }
   ],
   "source": [
    "for result in results[0:2]:\n",
    "    print(f\"---------------\\nQuestion:\\n{result[0]}\\n--------------- \\nAccepted Answer:\\n{result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d24745ff-902c-42b9-bd5e-ec5b46464a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_and_answers = _extract_questions_and_answers(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11b4378e-8e00-46f5-9bf4-1abd179e84ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'if/else in a list comprehension',\n",
       "  'accepted_answer': 'No answer body found.'},\n",
       " {'title': 'List comprehension vs. lambda + filter',\n",
       "  'accepted_answer': 'No answer body found.'},\n",
       " {'title': 'List comprehension vs map',\n",
       "  'accepted_answer': 'No answer body found.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_and_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8336c11-dd36-43a2-b896-8267ace1d149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
