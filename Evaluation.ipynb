{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a24129-0d25-4b94-9ee3-3874d2b1df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import json\n",
    "import git\n",
    "import os\n",
    "import validators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468f9fa-5a75-4bc9-9d28-1e886a0bf961",
   "metadata": {},
   "source": [
    "# Evaluating the LLM-Agen on SWE-Benchmark\n",
    "\n",
    "We have two datasets we can use for predicting `swe-bench.json` which has 2200 entries and `swe-bench-dev-dataset.json` which has 224 entries, they are from the [SWE-Bench](https://github.com/princeton-nlp/SWE-bench/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715d876a-03a8-4eaf-9c01-1bd2317d1b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 225 entries, 0 to 224\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   repo                      225 non-null    object             \n",
      " 1   instance_id               225 non-null    object             \n",
      " 2   base_commit               225 non-null    object             \n",
      " 3   patch                     225 non-null    object             \n",
      " 4   test_patch                225 non-null    object             \n",
      " 5   problem_statement         225 non-null    object             \n",
      " 6   hints_text                225 non-null    object             \n",
      " 7   created_at                225 non-null    datetime64[ns, UTC]\n",
      " 8   version                   225 non-null    float64            \n",
      " 9   FAIL_TO_PASS              225 non-null    object             \n",
      " 10  PASS_TO_PASS              225 non-null    object             \n",
      " 11  environment_setup_commit  225 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(10)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"SWEBench/swe-bench-dev-dataset.json\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41741fe9-a4d4-4f56-8f5c-cd0148ee7a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo                                                        sqlfluff/sqlfluff\n",
       "instance_id                                           sqlfluff__sqlfluff-4764\n",
       "base_commit                          a820c139ccbe6d1865d73c4a459945cd69899f8f\n",
       "patch                       diff --git a/src/sqlfluff/cli/commands.py b/sr...\n",
       "test_patch                  diff --git a/test/cli/commands_test.py b/test/...\n",
       "problem_statement           Enable quiet mode/no-verbose in CLI for use in...\n",
       "hints_text                                                                   \n",
       "created_at                                          2023-04-16 14:24:42+00:00\n",
       "version                                                                   1.4\n",
       "FAIL_TO_PASS                [test/cli/commands_test.py::test__cli__fix_mul...\n",
       "PASS_TO_PASS                [test/cli/commands_test.py::test__cli__command...\n",
       "environment_setup_commit             d19de0ecd16d298f9e3bfb91da122734c40c01e5\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b0387-5aa9-43be-9076-6e31eb5f58ed",
   "metadata": {},
   "source": [
    "After we used our LLM on the dataset to generate solutions to the problems, our output needs to be in the following format:\n",
    "```\n",
    "{\n",
    "    \"instance_id\": \"<Unique task instance ID>\",\n",
    "    \"model_patch\": \"<.patch file content string>\",\n",
    "    \"model_name_or_path\": \"<Model name here (i.e. SWE-Llama-13b)>\",\n",
    "}\n",
    "```\n",
    "With multiple prediction like this `[<prediction 1>, <prediction 2>,... <prediction n>]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cd217-25f3-4879-9a7c-b9fa7df0eb6e",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "```\n",
    "{\n",
    "    \"instance_id\": \"django__django-15127\",\n",
    "    \"model_name_or_path\": \"test\",\n",
    "    \"model_patch\": \"--- a/django/contrib/messages/storage/base.py\\n+++ b/django/contrib/messages/storage/base.py\\n@@ -52,6 +52,7 @@\\n                 if self._loaded_data is None:\\n                     self._loaded_data = self.load()\\n                 level, message, extra_tags = self._loaded_data\\n+                extra_tags.update(self.get_level_tags())\\n                 return {\\n                     'message': message,\\n                     'level': level,\\n\"\n",
    "  },\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999edc7-0d2b-4c6c-b55f-2dcf19ce7ed9",
   "metadata": {},
   "source": [
    "# Generating our Predictions\n",
    "\n",
    "## Defining the AgentWrapper\n",
    "\n",
    "We first define an `AgentWrapper` which job it is to:\n",
    "- clone the repos, set the head to the correct commit.\n",
    "- Calls our internal Agent, which does the changes.\n",
    "- Stages our changes.\n",
    "- Calculates the git diff, which we return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7884052-eba6-4753-95dd-c722d0bba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentStub():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, row_input, repo_dir):\n",
    "        new_file = os.path.join(repo_dir, 'test_file.md')\n",
    "        \n",
    "        fp = open(new_file, 'w+')\n",
    "        fp.write('This is a test file, which test if the git diff gets caluclated correctly.')\n",
    "        fp.close()\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "class AgentWrapper():\n",
    "    def __init__(self, agent, working_directory=\"repos\"):\n",
    "        self.name = \"stub\"\n",
    "        self.working_directory = working_directory\n",
    "        self.agent = agent\n",
    "\n",
    "        if not os.path.isdir(working_directory):\n",
    "            os.makedirs(working_directory)\n",
    "\n",
    "    def predict(self, row_input: Series):\n",
    "        repo_dir = self._clone_repo(row_input[\"repo\"], row_input[\"base_commit\"])\n",
    "        \n",
    "        result = self.agent.predict(row_input, repo_dir)\n",
    "\n",
    "        repo = git.Repo(repo_dir)\n",
    "        repo.git.add(\"*\")\n",
    "        return repo.git.diff(\"--cached\")\n",
    "\n",
    "    def _clone_repo(self, repo_name: str, base_commit: str):\n",
    "        repo_url = \"https://github.com/\" + repo_name\n",
    "        repo_dir = os.path.join(self.working_directory, repo_name.split('/', 1)[1])\n",
    "        \n",
    "        if not validators.url(repo_url):\n",
    "            raise Exception(\"The Repo url is not valid: \" + repo_url)\n",
    "                    \n",
    "        if not os.path.isdir(repo_dir):\n",
    "            os.makedirs(repo_dir)\n",
    "\n",
    "            # clones the repo on which llm will work\n",
    "            git.Repo.clone_from(repo_url, repo_dir)\n",
    "        \n",
    "        # we need to make sure we have the correct commit stage\n",
    "        repo = git.Repo(repo_dir)\n",
    "        repo.git.reset('--hard', base_commit)\n",
    "\n",
    "        return repo_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8f0c4-11cb-4a22-8bea-1598c434ca56",
   "metadata": {},
   "source": [
    "## Testing AgentWrapper\n",
    "\n",
    "Testing that the cloning mechanism for repos and checking out the correct git commit is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09db429b-8ea5-4d23-9760-52efd84c464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a820c139ccbe6d1865d73c4a459945cd69899f8f'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"base_commit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17212c24-03a6-4690-b1e4-921e68c5254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stub\n",
      "----------------\n",
      "diff --git a/test_file.md b/test_file.md\n",
      "new file mode 100644\n",
      "index 000000000..76c846906\n",
      "--- /dev/null\n",
      "+++ b/test_file.md\n",
      "@@ -0,0 +1 @@\n",
      "+This is a test file, which test if the git diff gets caluclated correctly.\n",
      "\\ No newline at end of file\n"
     ]
    }
   ],
   "source": [
    "stub = AgentStub()\n",
    "agent = AgentWrapper(stub, \"repos\")\n",
    "\n",
    "print(agent.name)\n",
    "print(\"----------------\")\n",
    "print(agent.predict(df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6494f-e13c-4e01-a860-86a8af311e54",
   "metadata": {},
   "source": [
    "## Generating all Predictions\n",
    "\n",
    "When running this on a server, it could happen that something crashed or an error is thrown which doesn't get catches, as such it is important to write the changes to disk for each entry in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55df9f9d-f75e-4417-9f5d-83e104ff0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation uses checkpoints, this means if the program \n",
    "# is interuppted it can start again, where it left oft.\n",
    "\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "stub = AgentStub()\n",
    "agent = AgentWrapper(stub, \"repos\")\n",
    "\n",
    "checkpoint_file = 'checkpoint.txt'\n",
    "resume_index = 0\n",
    "\n",
    "# Check if checkpoint file exists and read the last processed index\n",
    "try:\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        resume_index = int(f.read().strip())\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(f\"Error reading checkpoint file: {e}\")\n",
    "\n",
    "if resume_index < len(df) - 1:\n",
    "    # Open a file to save predictions\n",
    "    with open('predictions.json', 'a') as json_file:\n",
    "        if resume_index == 0:\n",
    "            json_file.write('[')  # Start of JSON array\n",
    "            json_file.write('\\n')\n",
    "        # Generating our solution\n",
    "        for index, row in df.iterrows():\n",
    "            # Skip rows that were already processed\n",
    "            if index < resume_index:\n",
    "                continue\n",
    "    \n",
    "            predictions = {\n",
    "                \"instance_id\": row[\"instance_id\"],\n",
    "                \"model_patch\": agent.predict(row),\n",
    "                \"model_name_or_path\": agent.name\n",
    "            }\n",
    "            # Convert the dictionary to a JSON formatted string and write to file\n",
    "            json_data = json.dumps(predictions, indent=4)\n",
    "            json_file.write(json_data)\n",
    "            if index < len(df) - 1:\n",
    "                json_file.write(',')\n",
    "            json_file.write('\\n')\n",
    "    \n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                f.write(str(index))\n",
    "                \n",
    "        if index == len(df) - 1:\n",
    "            json_file.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde4a12-5e22-4912-9d21-7d65dc8c5946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
