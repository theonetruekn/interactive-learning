{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a24129-0d25-4b94-9ee3-3874d2b1df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import git\n",
    "import os\n",
    "import validators\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468f9fa-5a75-4bc9-9d28-1e886a0bf961",
   "metadata": {},
   "source": [
    "# Evaluating the LLM-Agen on SWE-Benchmark\n",
    "\n",
    "We have two datasets we can use for predicting `swe-bench.json` which has 2200 entries and `swe-bench-lite.json` which has 224 entries, they are from the [SWE-Bench](https://github.com/princeton-nlp/SWE-bench/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715d876a-03a8-4eaf-9c01-1bd2317d1b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 225 entries, 0 to 224\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   repo                      225 non-null    object             \n",
      " 1   instance_id               225 non-null    object             \n",
      " 2   base_commit               225 non-null    object             \n",
      " 3   patch                     225 non-null    object             \n",
      " 4   test_patch                225 non-null    object             \n",
      " 5   problem_statement         225 non-null    object             \n",
      " 6   hints_text                225 non-null    object             \n",
      " 7   created_at                225 non-null    datetime64[ns, UTC]\n",
      " 8   version                   225 non-null    float64            \n",
      " 9   FAIL_TO_PASS              225 non-null    object             \n",
      " 10  PASS_TO_PASS              225 non-null    object             \n",
      " 11  environment_setup_commit  225 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(10)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"Evaluation/swe-bench-lite.json\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41741fe9-a4d4-4f56-8f5c-cd0148ee7a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo                                                        sqlfluff/sqlfluff\n",
       "instance_id                                           sqlfluff__sqlfluff-4764\n",
       "base_commit                          a820c139ccbe6d1865d73c4a459945cd69899f8f\n",
       "patch                       diff --git a/src/sqlfluff/cli/commands.py b/sr...\n",
       "test_patch                  diff --git a/test/cli/commands_test.py b/test/...\n",
       "problem_statement           Enable quiet mode/no-verbose in CLI for use in...\n",
       "hints_text                                                                   \n",
       "created_at                                          2023-04-16 14:24:42+00:00\n",
       "version                                                                   1.4\n",
       "FAIL_TO_PASS                [test/cli/commands_test.py::test__cli__fix_mul...\n",
       "PASS_TO_PASS                [test/cli/commands_test.py::test__cli__command...\n",
       "environment_setup_commit             d19de0ecd16d298f9e3bfb91da122734c40c01e5\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b0387-5aa9-43be-9076-6e31eb5f58ed",
   "metadata": {},
   "source": [
    "After we used our LLM on the dataset to generate solutions to the problems, our output needs to be in the following format:\n",
    "```\n",
    "{\n",
    "    \"instance_id\": \"<Unique task instance ID>\",\n",
    "    \"model_patch\": \"<.patch file content string>\",\n",
    "    \"model_name_or_path\": \"<Model name here (i.e. SWE-Llama-13b)>\",\n",
    "}\n",
    "```\n",
    "With multiple prediction like this `[<prediction 1>, <prediction 2>,... <prediction n>]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cd217-25f3-4879-9a7c-b9fa7df0eb6e",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "```\n",
    "{\n",
    "    \"instance_id\": \"django__django-15127\",\n",
    "    \"model_name_or_path\": \"test\",\n",
    "    \"model_patch\": \"--- a/django/contrib/messages/storage/base.py\\n+++ b/django/contrib/messages/storage/base.py\\n@@ -52,6 +52,7 @@\\n                 if self._loaded_data is None:\\n                     self._loaded_data = self.load()\\n                 level, message, extra_tags = self._loaded_data\\n+                extra_tags.update(self.get_level_tags())\\n                 return {\\n                     'message': message,\\n                     'level': level,\\n\"\n",
    "  },\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a82b1-185c-4b45-aad8-ed0efb059b5d",
   "metadata": {},
   "source": [
    "# Testing SmolCoder\n",
    "\n",
    "This requires starting the `phi3:latest` model, with ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a127113-b474-4e59-93f9-bb9c8f1ba82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lupos/miniconda3/envs/llm/lib/python311.zip', '/home/lupos/miniconda3/envs/llm/lib/python3.11', '/home/lupos/miniconda3/envs/llm/lib/python3.11/lib-dynload', '', '/home/lupos/miniconda3/envs/llm/lib/python3.11/site-packages', '/home/lupos/interactive-learning/SmolCoder']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(str(os.path.abspath('SmolCoder')))\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14072627-c353-4ee3-aaab-41fb32758e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from SmolCoder.src.agent import SmolCoder\n",
    "from SmolCoder.src.agent_wrapper import AgentWrapper\n",
    "from SmolCoder.src.llm_wrapper import LLM\n",
    "from SmolCoder.src.toolkit import Toolkit\n",
    "\n",
    "from SmolCoder.src.tools.list_methods import ListMethods\n",
    "from SmolCoder.src.tools.list_classes import ListClasses\n",
    "from SmolCoder.src.tools.list_files import ListFiles\n",
    "from SmolCoder.src.tools.replace_method import ReplaceMethod\n",
    "from SmolCoder.src.tools.finish import Finish\n",
    "from SmolCoder.src.tools.execute_python import ExecutePythonCode\n",
    "from SmolCoder.src.tools.show_method import ShowMethodBody\n",
    "from SmolCoder.src.tools.move_folder import MoveFolder\n",
    "from SmolCoder.src.tools.human_interaction import HumanInteraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237bac3c-59a8-4d40-ab75-0cc06c999082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Definition\n",
    "class_sumary = ListMethods()\n",
    "list_classes = ListClasses()\n",
    "list_files = ListFiles()\n",
    "replace_method = ReplaceMethod()\n",
    "finish = Finish()\n",
    "execute_python = ExecutePythonCode()\n",
    "show_method = ShowMethodBody()\n",
    "move_folder = MoveFolder()\n",
    "human_interaction = HumanInteraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8b706-bc7f-486b-8f55-63d47f19e693",
   "metadata": {},
   "source": [
    "## Testing Execute Python Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5b0734-bc24-40ff-9c95-6fd82f88d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = Toolkit([execute_python])\n",
    "\n",
    "agent = AgentWrapper(agent_name=\"SmolCoder\",\n",
    "                     toolkit=tools,\n",
    "                     mode=0,\n",
    "                     model=\"phi3:latest\",\n",
    "                     working_directory=\"repos\",\n",
    "                     logging_enabled=True\n",
    "                    )\n",
    "\n",
    "prompt = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0103363a-f353-4b72-9d19-ee9959167721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = agent.predict(prompt)\n",
    "#print(\"RESULT: \" + str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94d05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(smolCoder.inspect_history(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c78b30-5c0e-49c3-b605-366f1eb6f287",
   "metadata": {},
   "source": [
    "# SmolCoder on SWE\n",
    "\n",
    "This tests SmolCoder on a single Instance of the SWE-Benchmark.\n",
    "This is without first trying to reproduce the bug, just barebones ReAct with tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b74e473e-0e6b-4b57-a3b1-ed9c97686331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolkit = Toolkit([human_interaction, finish])\n",
    "toolkit = Toolkit([list_classes, list_files, replace_method, show_method, move_folder, finish])\n",
    "\n",
    "agent = AgentWrapper(\n",
    "                     agent_name=\"SmolCoder\",\n",
    "                     toolkit=toolkit,\n",
    "                     mode=0,\n",
    "                     model=\"phi3:latest\",\n",
    "                     working_directory=\"repos\",\n",
    "                     logging_enabled=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f9919-5077-45cd-8768-a8d96b6aa8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.name)\n",
    "print(\"----------------\")\n",
    "print(agent.predict(df.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19c91e-3fc0-471a-a479-c5bff68803fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(smol_coder.in# toolkit = Toolkit([human_interaction, finish])\n",
    "toolkit = Toolkit([human_interaction, list_classes, list_files, replace_method, show_method, move_folder, finish])\n",
    "\n",
    "agent = AgentWrapper(\n",
    "                     agent_name=\"SmolCoder\",\n",
    "                     toolkit=toolkit,\n",
    "                     mode=0,\n",
    "                     model=\"phi3:latest\",\n",
    "                     working_directory=\"repos\",\n",
    "                     logging_enabled=True\n",
    "                    )\n",
    "\n",
    "print(agent.name)\n",
    "print(\"----------------\")\n",
    "print(agent.predict(df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6494f-e13c-4e01-a860-86a8af311e54",
   "metadata": {},
   "source": [
    "## Generating all Predictions\n",
    "\n",
    "When running this on a server, it could happen that something crashed or an error is thrown which doesn't get catches, as such it is important to write the changes to disk for each entry in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df9f9d-f75e-4417-9f5d-83e104ff0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation uses checkpoints, this means if the program \n",
    "# is interuppted it can start again, where it left oft.\n",
    "\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "#tools = Toolkit([class_sumary, list_classes, list_files, finish])\n",
    "#model = LLM(\"phi3:latest\")\n",
    "#smol_coder = SmolCoder(model, Path(\"repos\"), tools)\n",
    "#agent = AgentWrapper(smol_coder, working_directory=\"repos\", name=\"SmolCoder\")\n",
    "\n",
    "stub = AgentStub()\n",
    "agent = AgentWrapper(stub, \"repos\")\n",
    "\n",
    "checkpoint_file = 'checkpoint.txt'\n",
    "resume_index = 0\n",
    "\n",
    "activated = 1\n",
    "\n",
    "if activated:\n",
    "    # Check if checkpoint file exists and read the last processed index\n",
    "    try:\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            resume_index = int(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading checkpoint file: {e}\")\n",
    "    \n",
    "    if resume_index < len(df) - 1:\n",
    "        # Open a file to save predictions\n",
    "        with open('predictions.json', 'a', encoding=\"utf-8-sig\") as json_file:\n",
    "            if resume_index == 0:\n",
    "                json_file.write('[')  # Start of JSON array\n",
    "                json_file.write('\\n')\n",
    "            # Generating our solution\n",
    "            for index, row in df.iterrows():\n",
    "                if index % 10 == 0: print(\"Current idx: \" + str(index))\n",
    "                # Skip rows that were already processed\n",
    "                if index < resume_index:\n",
    "                    continue\n",
    "        \n",
    "                predictions = {\n",
    "                    \"instance_id\": row[\"instance_id\"],\n",
    "                    \"model_patch\": agent.predict(row),\n",
    "                    \"model_name_or_path\": agent.name\n",
    "                }\n",
    "                # Convert the dictionary to a JSON formatted string and write to file\n",
    "                json_data = json.dumps(predictions, indent=4)\n",
    "                json_file.write(json_data)\n",
    "                if index < len(df) - 1:\n",
    "                    json_file.write(',')\n",
    "                json_file.write('\\n')\n",
    "        \n",
    "                with open(checkpoint_file, 'w') as f:\n",
    "                    f.write(str(index))\n",
    "                    \n",
    "            if index == len(df) - 1:\n",
    "                json_file.write(']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ff1e1",
   "metadata": {},
   "source": [
    "# Meta Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cbbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from SmolCoder.src.llm_wrapper import LLM\n",
    "from SmolCoder.src.prompting_strategy import PromptingStrategy\n",
    "from SmolCoder.src.toolkit import Toolkit\n",
    "from SmolCoder.src.tools.list_methods import ListMethods\n",
    "from SmolCoder.src.tools.list_files import ListFiles\n",
    "from SmolCoder.src.tools.list_classes import ListClasses\n",
    "from SmolCoder.src.tools.finish import Finish\n",
    "from SmolCoder.src.meta_tokenizer import MetaTokenizer\n",
    "\n",
    "from SmolCoder.src.agent import SmolCoder\n",
    "\n",
    "list_methods = ListMethods()\n",
    "list_classes = ListClasses()\n",
    "list_files = ListFiles()\n",
    "finish = Finish()\n",
    "\n",
    "toolkit = Toolkit([list_methods, list_classes, list_files, finish])\n",
    "\n",
    "smol = SmolCoder(model=LLM(\"phi3:latest\", openai=[False, \"None\"], logger=None), codebase_dir= Path(\"test_codebase/\"), toolkit=toolkit, logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31b2f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Sysprompt]You will be given `question` and you will respond with `answer`.\\n\\nTry to think step for step, do NOT take steps that are too big.\\nTo do this, you will interleave Thought, Action, and Observation steps.\\nThought can reason about the current situation.\\n\\nAction can be the following types, \\n(1) List_Methods[class_name], which lists the signatures and docstring of all the method of the class `class_name`. Example use: List_Methods[MyClass]\\n(2) List_Classes[file_name], which lists all class names and their docstring that are in the Python file file_name. Example use: List_Classes[test.py]\\n(3) List_Files[folder], which lists all the files and subfolder that are in the current folder. Example use: List_Files[some_dir] or List_Files[.] to list the files of the current directory\\n(4) Finish[answer], which finishes the program and returns the answer. Example use: Finish[\"The Answer is 42.\"]\\n\\nInput variables of the tools do not need quotation marks around them. In addition, do NOT use the `finish` tool before having made all changes to remedy the issue.\\n\\nThe Example is now finished.During your execution, follow the following format:\\n\\n[Thought]Reasoning which action to take to solve the task.\\n[Action]Always either List_Methods[class_name] or List_Classes[file_name] or List_Files[folder] or Finish[answer]\\n[Observation]result of the previous Action\\n[Thought]next steps to take based on the previous Observation\\n...\\nuntil [Action] is of type `Finish`.\\nDo not use any special formatation such as markdown.\\nThe [Observation] will automatically be given to you after you used an action, so you can stop after taking an [Action]---\\n\\n[Sysprompt][Question]What methods do the classes in test.py have?\\n[Thought]To find out what methods are available for the classes defined in `test.py`, I will list all the classes present in this file and then look up their method signatures using List_Methods with each class name.\\n\\n[Action]List_Classes[\\'test.py\\']\\n\\n[Observation]The classes in `test.py` are `MyClass` with docstring `No docstring provided`, `MyClass2` with docstring `No docstring provided`.\\n(Current Working Directory: test_codebase) \\n\\n[Thought]With the class names obtained from List_Classes[\\'test.py\\', I will now list the methods for each of these classes using List_Methods on their respective names.\\n[Action]List_Methods[MyClass]\\n\\n[Observation]Method `__init__(self, one, two)` with docstring `{ Init the class. Careful: do not touch! }`,\\nMethod `do_stuff()` with docstring `{ None }`\\n(Current Working Directory: test_codebase) \\n\\n[Thought]With the method signatures for `MyClass` obtained from List_Methods[MyClass], I will now list methods for another class, `MyClass2`.\\n[Action]List_Methods[MyClass2]\\n\\n[Observation]Method `another_way_to_do_stuff(self, i)` with docstring `{ None }`\\n(Current Working Directory: test_codebase) \\n\\n[Thought]The classes in the file `test.py` have several methods associated with them. I will list all these method signatures for each class to provide a comprehensive answer.\\n[Action]List_Methods[MyClass2]\\n\\n[Observation]Method `another_way_to_do_stuff(self, i)` with docstring `{ None }`\\n(Current Working Directory: test_codebase) \\n\\n[Thought]Based on the results of List_Methods for both classes, I can now provide a list of methods that are available in these two classes.\\n[Action]Finish[\"The methods available in `MyClass` and `MyClass2` include `__init__`, `do_stuff()`, and `another_way_to_do_stuff(self, i)`.\"]The methods available in `MyClass` and `MyClass2` include `__init__`, `do_stuff()`, and `another_way_to_do_stuff(self, i)`.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smol(\"[Question]What methods do the classes in test.py have?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
